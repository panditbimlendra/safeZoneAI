{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba05a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAbnormal Sound Detection model\\nDetects car crashes, accidents, gunshots, and other abnormal sounds\\nAuthor: AI Assistant\\nDate: 2024\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Abnormal Sound Detection model\n",
    "Detects car crashes, accidents, gunshots, and other abnormal sounds\n",
    "Author: AI Assistant\n",
    "Date: 2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f23fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import wave\n",
    "import scipy.io.wavfile as wavf\n",
    "from scipy import signal\n",
    "from scipy.stats import skew, kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           accuracy_score, precision_recall_fscore_support,\n",
    "                           roc_auc_score, roc_curve)\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50169a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow not available. Deep learning models will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For deep learning\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models, callbacks\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"TensorFlow not available. Deep learning models will be disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afdf657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.SoundDetectionConfig'>\n",
      "{'SAMPLE_RATE': 22050, 'DURATION': 4.0, 'N_MFCC': 40, 'N_MELS': 128, 'N_FFT': 2048, 'HOP_LENGTH': 512, 'ZCR_FRAME_LENGTH': 2048, 'ZCR_HOP_LENGTH': 512, 'RMS_FRAME_LENGTH': 2048, 'RMS_HOP_LENGTH': 512, 'TEST_SIZE': 0.2, 'VAL_SIZE': 0.1, 'RANDOM_STATE': 42, 'CLASSES': ['background', 'bumping', 'speech', 'gunshot', 'car_crash', 'scream', 'explosion', 'glass_breaking', 'alarm'], 'USE_MFCC': True, 'USE_MEL_SPECTROGRAM': True, 'USE_CHROMA': True, 'USE_TONNETZ': True, 'USE_SPECTRAL_FEATURES': True, 'USE_ZCR': True, 'USE_RMS': True, 'USE_TEMPORAL_FEATURES': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SoundDetectionConfig:\n",
    "    \"\"\"Configuration parameters for the sound detection system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Audio processing\n",
    "        self.SAMPLE_RATE = 22050  # Standard for sound detection\n",
    "        self.DURATION = 4.0  # Duration of each audio clip in seconds\n",
    "        self.N_MFCC = 40  # Number of MFCC coefficients\n",
    "        self.N_MELS = 128  # Number of mel bands\n",
    "        self.N_FFT = 2048  # FFT window size\n",
    "        self.HOP_LENGTH = 512  # Hop length for STFT\n",
    "        \n",
    "        # Feature extraction\n",
    "        self.ZCR_FRAME_LENGTH = 2048\n",
    "        self.ZCR_HOP_LENGTH = 512\n",
    "        self.RMS_FRAME_LENGTH = 2048\n",
    "        self.RMS_HOP_LENGTH = 512\n",
    "        \n",
    "        # Model parameters\n",
    "        self.TEST_SIZE = 0.2\n",
    "        self.VAL_SIZE = 0.1\n",
    "        self.RANDOM_STATE = 42\n",
    "        \n",
    "        # Classes to detect\n",
    "        self.CLASSES = [\n",
    "            'background',  # 0\n",
    "            'bumping',     # 1 - from your original data\n",
    "            'speech',      # 2 - from your original data\n",
    "            'gunshot',     # 3 - new\n",
    "            'car_crash',   # 4 - new\n",
    "            'scream',      # 5 - new\n",
    "            'explosion',   # 6 - new\n",
    "            'glass_breaking', # 7 - new\n",
    "            'alarm'        # 8 - new\n",
    "        ]\n",
    "        \n",
    "        # Feature selection\n",
    "        self.USE_MFCC = True\n",
    "        self.USE_MEL_SPECTROGRAM = True\n",
    "        self.USE_CHROMA = True\n",
    "        self.USE_TONNETZ = True\n",
    "        self.USE_SPECTRAL_FEATURES = True\n",
    "        self.USE_ZCR = True\n",
    "        self.USE_RMS = True\n",
    "        self.USE_TEMPORAL_FEATURES = True\n",
    "\n",
    "config = SoundDetectionConfig()\n",
    "print(type(config))\n",
    "print(vars(config))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "431f6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedAudioFeatureExtractor:\n",
    "    \"\"\"Extracts comprehensive audio features for abnormal sound detection\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.sample_rate = config.sample_rate\n",
    "        self.n_mfcc = config.n_mfcc\n",
    "        self.n_fft = config.n_fft\n",
    "        self.hop_length = config.hop_length\n",
    "        \n",
    "    def extract_all_features(self, audio_path=None, audio_array=None, sr=None):\n",
    "        \"\"\"\n",
    "        Extract all audio features from a file or array\n",
    "        \"\"\"\n",
    "        if audio_path:\n",
    "            audio, sr = librosa.load(audio_path, sr=self.config.SAMPLE_RATE, \n",
    "                                   duration=self.config.DURATION)\n",
    "        elif audio_array is not None:\n",
    "            audio = audio_array\n",
    "            if sr is None:\n",
    "                sr = self.config.SAMPLE_RATE\n",
    "        else:\n",
    "            raise ValueError(\"Either audio_path or audio_array must be provided\")\n",
    "        \n",
    "        # Ensure audio is proper length\n",
    "        target_length = int(self.config.DURATION * sr)\n",
    "        if len(audio) > target_length:\n",
    "            audio = audio[:target_length]\n",
    "        elif len(audio) < target_length:\n",
    "            audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # 1. MFCC Features (your existing approach, enhanced)\n",
    "        if self.config.USE_MFCC:\n",
    "            mfccs = librosa.feature.mfcc(y=audio, sr=sr, \n",
    "                                        n_mfcc=self.config.N_MFCC,\n",
    "                                        n_fft=self.config.N_FFT,\n",
    "                                        hop_length=self.config.HOP_LENGTH)\n",
    "            features['mfcc_mean'] = np.mean(mfccs, axis=1)\n",
    "            features['mfcc_std'] = np.std(mfccs, axis=1)\n",
    "            features['mfcc_skew'] = skew(mfccs, axis=1)\n",
    "            features['mfcc_delta'] = np.mean(librosa.feature.delta(mfccs), axis=1)\n",
    "            features['mfcc_delta2'] = np.mean(librosa.feature.delta(mfccs, order=2), axis=1)\n",
    "        \n",
    "        # 2. Mel-Spectrogram Features\n",
    "        if self.config.USE_MEL_SPECTROGRAM:\n",
    "            mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr,\n",
    "                                                    n_mels=self.config.N_MELS,\n",
    "                                                    n_fft=self.config.N_FFT,\n",
    "                                                    hop_length=self.config.HOP_LENGTH)\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            features['mel_mean'] = np.mean(mel_spec_db, axis=1)\n",
    "            features['mel_std'] = np.std(mel_spec_db, axis=1)\n",
    "            features['mel_flux'] = np.mean(np.diff(mel_spec_db, axis=1), axis=1)\n",
    "        \n",
    "        # 3. Chroma Features (for harmonic content)\n",
    "        if self.config.USE_CHROMA:\n",
    "            chroma_stft = librosa.feature.chroma_stft(y=audio, sr=sr,\n",
    "                                                    n_fft=self.config.N_FFT,\n",
    "                                                    hop_length=self.config.HOP_LENGTH)\n",
    "            features['chroma_mean'] = np.mean(chroma_stft, axis=1)\n",
    "            features['chroma_std'] = np.std(chroma_stft, axis=1)\n",
    "            features['chroma_cqt'] = np.mean(librosa.feature.chroma_cqt(y=audio, sr=sr), axis=1)\n",
    "        \n",
    "        # 4. Spectral Features\n",
    "        if self.config.USE_SPECTRAL_FEATURES:\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "            spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
    "            \n",
    "            features['spectral_centroid'] = np.mean(spectral_centroid)\n",
    "            features['spectral_bandwidth'] = np.mean(spectral_bandwidth)\n",
    "            features['spectral_rolloff'] = np.mean(spectral_rolloff)\n",
    "            features['spectral_contrast'] = np.mean(spectral_contrast, axis=1)\n",
    "            \n",
    "            # Zero-crossing rate (important for abrupt sounds)\n",
    "            if self.config.USE_ZCR:\n",
    "                zcr = librosa.feature.zero_crossing_rate(y=audio,\n",
    "                                                        frame_length=self.config.ZCR_FRAME_LENGTH,\n",
    "                                                        hop_length=self.config.ZCR_HOP_LENGTH)\n",
    "                features['zcr_mean'] = np.mean(zcr)\n",
    "                features['zcr_std'] = np.std(zcr)\n",
    "                features['zcr_max'] = np.max(zcr)\n",
    "        \n",
    "        # 5. RMS Energy (for loudness detection)\n",
    "        if self.config.USE_RMS:\n",
    "            rms = librosa.feature.rms(y=audio,\n",
    "                                    frame_length=self.config.RMS_FRAME_LENGTH,\n",
    "                                    hop_length=self.config.RMS_HOP_LENGTH)\n",
    "            features['rms_mean'] = np.mean(rms)\n",
    "            features['rms_std'] = np.std(rms)\n",
    "            features['rms_max'] = np.max(rms)\n",
    "            features['rms_ratio'] = np.max(rms) / (np.mean(rms) + 1e-8)  # Peak-to-average ratio\n",
    "        \n",
    "        # 6. Temporal Features\n",
    "        if self.config.USE_TEMPORAL_FEATURES:\n",
    "            # Attack time (time to reach peak)\n",
    "            envelope = np.abs(librosa.core.stft(audio, n_fft=1024, hop_length=256))\n",
    "            envelope = np.mean(envelope, axis=0)\n",
    "            peak_idx = np.argmax(envelope)\n",
    "            attack_time = peak_idx / (sr / 256)  # Convert to seconds\n",
    "            features['attack_time'] = attack_time\n",
    "            \n",
    "            # Decay time (time from peak to sustain level)\n",
    "            sustain_level = np.mean(envelope[peak_idx:peak_idx + 10])\n",
    "            decay_threshold = 0.1  # 10% of peak\n",
    "            decay_idx = np.where(envelope[peak_idx:] < decay_threshold * np.max(envelope))[0]\n",
    "            if len(decay_idx) > 0:\n",
    "                decay_time = decay_idx[0] / (sr / 256)\n",
    "            else:\n",
    "                decay_time = len(envelope) - peak_idx / (sr / 256)\n",
    "            features['decay_time'] = decay_time\n",
    "        \n",
    "        # 7. Tonnetz Features (tonal content)\n",
    "        if self.config.USE_TONNETZ:\n",
    "            tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(audio), sr=sr)\n",
    "            features['tonnetz_mean'] = np.mean(tonnetz, axis=1)\n",
    "        \n",
    "        # 8. Additional statistical features\n",
    "        features['kurtosis'] = kurtosis(audio)\n",
    "        features['skewness'] = skew(audio)\n",
    "        features['crest_factor'] = np.max(np.abs(audio)) / (np.sqrt(np.mean(audio**2)) + 1e-8)\n",
    "        \n",
    "        # Flatten all features into a single vector\n",
    "        feature_vector = []\n",
    "        for key in sorted(features.keys()):\n",
    "            if isinstance(features[key], np.ndarray):\n",
    "                feature_vector.extend(features[key].flatten())\n",
    "            else:\n",
    "                feature_vector.append(features[key])\n",
    "        \n",
    "        return np.array(feature_vector), features\n",
    "    \n",
    "    def create_feature_dataframe(self, audio_paths, labels):\n",
    "        \"\"\"\n",
    "        Create a DataFrame of features from multiple audio files\n",
    "        \"\"\"\n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        print(\"Extracting features from audio files...\")\n",
    "        for i, (path, label) in enumerate(zip(audio_paths, labels)):\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Processing file {i}/{len(audio_paths)}...\")\n",
    "            \n",
    "            try:\n",
    "                features, _ = self.extract_all_features(audio_path=path)\n",
    "                features_list.append(features)\n",
    "                labels_list.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Create DataFrame\n",
    "        X = np.vstack(features_list)\n",
    "        y = np.array(labels_list)\n",
    "        \n",
    "        # Normalize features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        return X_scaled, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f051090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. DATA PREPROCESSING AND AUGMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "class AudioDataPreprocessor:\n",
    "    \"\"\"Handles audio data preprocessing and augmentation\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def augment_audio(audio, sr, augment_type='all'):\n",
    "        \"\"\"\n",
    "        Apply audio augmentation techniques\n",
    "        \"\"\"\n",
    "        augmented = audio.copy()\n",
    "        \n",
    "        if augment_type == 'all' or augment_type == 'noise':\n",
    "            # Add Gaussian noise\n",
    "            noise = np.random.randn(len(audio)) * 0.005 * np.max(audio)\n",
    "            augmented = augmented + noise\n",
    "        \n",
    "        if augment_type == 'all' or augment_type == 'shift':\n",
    "            # Time shifting\n",
    "            shift = int(np.random.uniform(-sr//10, sr//10))\n",
    "            if shift > 0:\n",
    "                augmented = np.pad(augmented, (shift, 0), mode='constant')[0:len(audio)]\n",
    "            else:\n",
    "                augmented = np.pad(augmented, (0, -shift), mode='constant')[shift:]\n",
    "        \n",
    "        if augment_type == 'all' or augment_type == 'pitch':\n",
    "            # Pitch shifting (Â±2 semitones)\n",
    "            n_steps = np.random.uniform(-2, 2)\n",
    "            augmented = librosa.effects.pitch_shift(augmented, sr=sr, n_steps=n_steps)\n",
    "        \n",
    "        if augment_type == 'all' or augment_type == 'speed':\n",
    "            # Speed perturbation (0.9x to 1.1x)\n",
    "            speed_factor = np.random.uniform(0.9, 1.1)\n",
    "            augmented = librosa.effects.time_stretch(augmented, rate=speed_factor)\n",
    "            \n",
    "            # Ensure same length\n",
    "            if len(augmented) > len(audio):\n",
    "                augmented = augmented[:len(audio)]\n",
    "            else:\n",
    "                augmented = np.pad(augmented, (0, len(audio) - len(augmented)), mode='constant')\n",
    "        \n",
    "        return augmented\n",
    "    \n",
    "    @staticmethod\n",
    "    def balance_dataset(X, y, target_samples_per_class=1000):\n",
    "        \"\"\"\n",
    "        Balance dataset using oversampling and undersampling\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        \n",
    "        class_counts = Counter(y)\n",
    "        print(f\"Original class distribution: {class_counts}\")\n",
    "        \n",
    "        # Oversample minority classes\n",
    "        smote = SMOTE(random_state=42, sampling_strategy='auto')\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "        \n",
    "        # Undersample majority classes if still imbalanced\n",
    "        rus = RandomUnderSampler(random_state=42, sampling_strategy='auto')\n",
    "        X_balanced, y_balanced = rus.fit_resample(X_resampled, y_resampled)\n",
    "        \n",
    "        balanced_counts = Counter(y_balanced)\n",
    "        print(f\"Balanced class distribution: {balanced_counts}\")\n",
    "        \n",
    "        return X_balanced, y_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e029d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 4. MODEL TRAINING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "class AbnormalSoundDetector:\n",
    "    \"\"\"Main class for abnormal sound detection\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.feature_extractor = AdvancedAudioFeatureExtractor(config)\n",
    "        self.preprocessor = AudioDataPreprocessor()\n",
    "        self.models = {}\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def prepare_data(self, data_directory=None, audio_paths=None, labels=None):\n",
    "        \"\"\"\n",
    "        Prepare data for training\n",
    "        \"\"\"\n",
    "        if data_directory:\n",
    "            # Load data from directory structure\n",
    "            audio_paths = []\n",
    "            labels = []\n",
    "            \n",
    "            for class_idx, class_name in enumerate(self.config.CLASSES):\n",
    "                class_dir = os.path.join(data_directory, class_name)\n",
    "                if os.path.exists(class_dir):\n",
    "                    for file in os.listdir(class_dir):\n",
    "                        if file.endswith(('.wav', '.mp3', '.flac')):\n",
    "                            audio_paths.append(os.path.join(class_dir, file))\n",
    "                            labels.append(class_idx)\n",
    "        \n",
    "        # Extract features\n",
    "        X, y = self.feature_extractor.create_feature_dataframe(audio_paths, labels)\n",
    "        \n",
    "        # Encode labels\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_encoded, test_size=self.config.TEST_SIZE, \n",
    "            random_state=self.config.RANDOM_STATE, stratify=y_encoded\n",
    "        )\n",
    "        \n",
    "        # Further split for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=self.config.VAL_SIZE, \n",
    "            random_state=self.config.RANDOM_STATE, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        print(f\"Training samples: {X_train.shape[0]}\")\n",
    "        print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "        print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "    def train_ensemble_model(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Train an ensemble of models for better performance\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING ENSEMBLE MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. Random Forest (your existing model, enhanced)\n",
    "        print(\"\\n1. Training Random Forest...\")\n",
    "        rf_params = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'class_weight': ['balanced', 'balanced_subsample']\n",
    "        }\n",
    "        \n",
    "        rf = RandomForestClassifier(random_state=self.config.RANDOM_STATE, n_jobs=-1)\n",
    "        rf_grid = GridSearchCV(rf, rf_params, cv=3, scoring='f1_weighted', verbose=1)\n",
    "        rf_grid.fit(X_train, y_train)\n",
    "        self.models['random_forest'] = rf_grid.best_estimator_\n",
    "        print(f\"Best RF params: {rf_grid.best_params_}\")\n",
    "        print(f\"RF Validation Score: {rf_grid.best_score_:.4f}\")\n",
    "        \n",
    "        # 2. Gradient Boosting\n",
    "        print(\"\\n2. Training Gradient Boosting...\")\n",
    "        gb_params = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        }\n",
    "        \n",
    "        gb = GradientBoostingClassifier(random_state=self.config.RANDOM_STATE)\n",
    "        gb_grid = GridSearchCV(gb, gb_params, cv=3, scoring='f1_weighted', verbose=1)\n",
    "        gb_grid.fit(X_train, y_train)\n",
    "        self.models['gradient_boosting'] = gb_grid.best_estimator_\n",
    "        print(f\"Best GB params: {gb_grid.best_params_}\")\n",
    "        print(f\"GB Validation Score: {gb_grid.best_score_:.4f}\")\n",
    "        \n",
    "        # 3. Support Vector Machine\n",
    "        print(\"\\n3. Training SVM...\")\n",
    "        svm_params = {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'kernel': ['rbf', 'poly'],\n",
    "            'gamma': ['scale', 'auto'],\n",
    "            'class_weight': ['balanced']\n",
    "        }\n",
    "        \n",
    "        svc = SVC(random_state=self.config.RANDOM_STATE, probability=True)\n",
    "        svm_grid = GridSearchCV(svc, svm_params, cv=3, scoring='f1_weighted', verbose=1)\n",
    "        svm_grid.fit(X_train, y_train)\n",
    "        self.models['svm'] = svm_grid.best_estimator_\n",
    "        print(f\"Best SVM params: {svm_grid.best_params_}\")\n",
    "        print(f\"SVM Validation Score: {svm_grid.best_score_:.4f}\")\n",
    "        \n",
    "        # 4. Neural Network\n",
    "        print(\"\\n4. Training Neural Network...\")\n",
    "        nn_params = {\n",
    "            'hidden_layer_sizes': [(100,), (100, 50), (200, 100)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'learning_rate_init': [0.001, 0.01]\n",
    "        }\n",
    "        \n",
    "        nn = MLPClassifier(random_state=self.config.RANDOM_STATE, max_iter=500)\n",
    "        nn_grid = GridSearchCV(nn, nn_params, cv=3, scoring='f1_weighted', verbose=1)\n",
    "        nn_grid.fit(X_train, y_train)\n",
    "        self.models['neural_network'] = nn_grid.best_estimator_\n",
    "        print(f\"Best NN params: {nn_grid.best_params_}\")\n",
    "        print(f\"NN Validation Score: {nn_grid.best_score_:.4f}\")\n",
    "        \n",
    "        # 5. Deep Learning Model (if TensorFlow available)\n",
    "        if TF_AVAILABLE and len(X_train) > 1000:\n",
    "            print(\"\\n5. Training Deep Learning Model...\")\n",
    "            self.models['deep_learning'] = self._build_deep_learning_model(X_train.shape[1], \n",
    "                                                                         len(self.config.CLASSES))\n",
    "            self._train_deep_learning_model(self.models['deep_learning'], \n",
    "                                          X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"VALIDATION SET PERFORMANCE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            if name != 'deep_learning':  # Deep learning evaluated separately\n",
    "                y_val_pred = model.predict(X_val)\n",
    "                accuracy = accuracy_score(y_val, y_val_pred)\n",
    "                print(f\"{name.upper()}: Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    def _build_deep_learning_model(self, input_dim, num_classes):\n",
    "        \"\"\"Build a deep neural network for sound classification\"\"\"\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(256, activation='relu', input_dim=input_dim),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            \n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            \n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.2),\n",
    "            \n",
    "            layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _train_deep_learning_model(self, model, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train the deep learning model\"\"\"\n",
    "        early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, \n",
    "                                                restore_best_weights=True)\n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                               patience=5, min_lr=0.00001)\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def evaluate_ensemble(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the ensemble of models\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENSEMBLE EVALUATION ON TEST SET\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        predictions = {}\n",
    "        probabilities = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            if name == 'deep_learning' and TF_AVAILABLE:\n",
    "                proba = model.predict(X_test)\n",
    "                predictions[name] = np.argmax(proba, axis=1)\n",
    "                probabilities[name] = proba\n",
    "            elif name != 'deep_learning':\n",
    "                predictions[name] = model.predict(X_test)\n",
    "                probabilities[name] = model.predict_proba(X_test)\n",
    "        \n",
    "        # 1. Individual model performance\n",
    "        print(\"\\nINDIVIDUAL MODEL PERFORMANCE:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for name, pred in predictions.items():\n",
    "            accuracy = accuracy_score(y_test, pred)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(y_test, pred, \n",
    "                                                                      average='weighted')\n",
    "            print(f\"\\n{name.upper()}:\")\n",
    "            print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall:    {recall:.4f}\")\n",
    "            print(f\"  F1-Score:  {f1:.4f}\")\n",
    "            \n",
    "            # Detailed classification report\n",
    "            print(\"\\n  Classification Report:\")\n",
    "            report = classification_report(y_test, pred, \n",
    "                                         target_names=self.config.CLASSES)\n",
    "            for line in report.split('\\n'):\n",
    "                print(f\"    {line}\")\n",
    "        \n",
    "        # 2. Ensemble prediction (voting)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENSEMBLE VOTING PERFORMANCE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Weighted voting based on validation performance\n",
    "        ensemble_pred = np.zeros((len(X_test), len(self.config.CLASSES)))\n",
    "        \n",
    "        for name, proba in probabilities.items():\n",
    "            if name == 'random_forest':\n",
    "                weight = 0.35  # RF usually performs well\n",
    "            elif name == 'gradient_boosting':\n",
    "                weight = 0.25\n",
    "            elif name == 'svm':\n",
    "                weight = 0.20\n",
    "            elif name == 'neural_network':\n",
    "                weight = 0.15\n",
    "            elif name == 'deep_learning':\n",
    "                weight = 0.05\n",
    "            else:\n",
    "                weight = 0.10\n",
    "            \n",
    "            ensemble_pred += weight * proba\n",
    "        \n",
    "        final_predictions = np.argmax(ensemble_pred, axis=1)\n",
    "        \n",
    "        # Evaluate ensemble\n",
    "        accuracy = accuracy_score(y_test, final_predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, final_predictions,\n",
    "                                                                  average='weighted')\n",
    "        \n",
    "        print(f\"\\nENSEMBLE RESULTS:\")\n",
    "        print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1-Score:  {f1:.4f}\")\n",
    "        \n",
    "        # Confusion matrix\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        cm = confusion_matrix(y_test, final_predictions)\n",
    "        self._plot_confusion_matrix(cm)\n",
    "        \n",
    "        # Detailed classification report\n",
    "        print(\"\\nDetailed Classification Report:\")\n",
    "        print(classification_report(y_test, final_predictions, \n",
    "                                  target_names=self.config.CLASSES))\n",
    "        \n",
    "        return final_predictions, ensemble_pred\n",
    "    \n",
    "    def _plot_confusion_matrix(self, cm):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.config.CLASSES,\n",
    "                   yticklabels=self.config.CLASSES)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict_single_audio(self, audio_path, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Predict class for a single audio file\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        features, _ = self.feature_extractor.extract_all_features(audio_path=audio_path)\n",
    "        features = self.feature_extractor.scaler.transform(features.reshape(1, -1))\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        probabilities = []\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            if name == 'deep_learning' and TF_AVAILABLE:\n",
    "                proba = model.predict(features)[0]\n",
    "            elif name != 'deep_learning':\n",
    "                proba = model.predict_proba(features)[0]\n",
    "            else:\n",
    "                continue\n",
    "            probabilities.append(proba)\n",
    "        \n",
    "        # Weighted ensemble probability\n",
    "        weights = [0.35, 0.25, 0.20, 0.15, 0.05]  # Same as in evaluate_ensemble\n",
    "        ensemble_proba = np.zeros(len(self.config.CLASSES))\n",
    "        \n",
    "        for i, proba in enumerate(probabilities[:len(weights)]):\n",
    "            ensemble_proba += weights[i] * proba\n",
    "        \n",
    "        # Get prediction\n",
    "        predicted_class_idx = np.argmax(ensemble_proba)\n",
    "        confidence = ensemble_proba[predicted_class_idx]\n",
    "        \n",
    "        # Check if confidence meets threshold\n",
    "        if confidence < threshold:\n",
    "            print(f\"Warning: Low confidence ({confidence:.2f}). Sound may be ambiguous.\")\n",
    "        \n",
    "        predicted_class = self.config.CLASSES[predicted_class_idx]\n",
    "        \n",
    "        # Get top 3 predictions\n",
    "        top_indices = np.argsort(ensemble_proba)[-3:][::-1]\n",
    "        top_classes = [self.config.CLASSES[i] for i in top_indices]\n",
    "        top_confidences = [ensemble_proba[i] for i in top_indices]\n",
    "        \n",
    "        return {\n",
    "            'predicted_class': predicted_class,\n",
    "            'confidence': confidence,\n",
    "            'top_predictions': list(zip(top_classes, top_confidences)),\n",
    "            'all_probabilities': dict(zip(self.config.CLASSES, ensemble_proba))\n",
    "        }\n",
    "    \n",
    "    def real_time_detection(self, audio_stream, window_duration=1.0, overlap=0.5):\n",
    "        \"\"\"\n",
    "        Real-time abnormal sound detection from audio stream\n",
    "        \"\"\"\n",
    "        import queue\n",
    "        import threading\n",
    "        \n",
    "        sr = self.config.SAMPLE_RATE\n",
    "        window_samples = int(window_duration * sr)\n",
    "        hop_samples = int((1 - overlap) * window_samples)\n",
    "        \n",
    "        # Buffer for audio stream\n",
    "        audio_buffer = queue.Queue()\n",
    "        detection_results = queue.Queue()\n",
    "        \n",
    "        def process_audio():\n",
    "            while True:\n",
    "                # Get audio chunk from buffer\n",
    "                audio_chunk = audio_buffer.get()\n",
    "                if audio_chunk is None:  # Termination signal\n",
    "                    break\n",
    "                \n",
    "                # Extract features and predict\n",
    "                features, _ = self.feature_extractor.extract_all_features(\n",
    "                    audio_array=audio_chunk, sr=sr\n",
    "                )\n",
    "                features = self.feature_extractor.scaler.transform(features.reshape(1, -1))\n",
    "                \n",
    "                # Get ensemble prediction\n",
    "                ensemble_proba = np.zeros(len(self.config.CLASSES))\n",
    "                weights = [0.35, 0.25, 0.20, 0.15, 0.05]\n",
    "                \n",
    "                for i, (name, model) in enumerate(self.models.items()):\n",
    "                    if i >= len(weights):\n",
    "                        break\n",
    "                    \n",
    "                    if name == 'deep_learning' and TF_AVAILABLE:\n",
    "                        proba = model.predict(features)[0]\n",
    "                    elif name != 'deep_learning':\n",
    "                        proba = model.predict_proba(features)[0]\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    ensemble_proba += weights[i] * proba\n",
    "                \n",
    "                # Check for abnormal sounds (classes 3-8)\n",
    "                abnormal_classes = self.config.CLASSES[3:]\n",
    "                abnormal_indices = [self.config.CLASSES.index(c) for c in abnormal_classes]\n",
    "                abnormal_prob = np.sum(ensemble_proba[abnormal_indices])\n",
    "                \n",
    "                if abnormal_prob > 0.5:  # Threshold for abnormality\n",
    "                    predicted_idx = np.argmax(ensemble_proba)\n",
    "                    predicted_class = self.config.CLASSES[predicted_idx]\n",
    "                    confidence = ensemble_proba[predicted_idx]\n",
    "                    \n",
    "                    detection_results.put({\n",
    "                        'timestamp': time.time(),\n",
    "                        'class': predicted_class,\n",
    "                        'confidence': confidence,\n",
    "                        'abnormal': True,\n",
    "                        'all_probabilities': ensemble_proba\n",
    "                    })\n",
    "                \n",
    "                audio_buffer.task_done()\n",
    "        \n",
    "        # Start processing thread\n",
    "        processor_thread = threading.Thread(target=process_audio)\n",
    "        processor_thread.start()\n",
    "        \n",
    "        # Return buffers for external use\n",
    "        return audio_buffer, detection_results, processor_thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96ed9944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. VISUALIZATION AND ANALYSIS TOOLS\n",
    "# ============================================================================\n",
    "\n",
    "class SoundVisualizer:\n",
    "    \"\"\"Visualization tools for audio analysis\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_audio_features(audio_path, config):\n",
    "        \"\"\"Plot various audio features for analysis\"\"\"\n",
    "        audio, sr = librosa.load(audio_path, sr=config.SAMPLE_RATE)\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Waveform\n",
    "        axes[0, 0].plot(np.arange(len(audio)) / sr, audio)\n",
    "        axes[0, 0].set_title('Waveform')\n",
    "        axes[0, 0].set_xlabel('Time (s)')\n",
    "        axes[0, 0].set_ylabel('Amplitude')\n",
    "        \n",
    "        # 2. Spectrogram\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "        img = librosa.display.specshow(D, y_axis='log', x_axis='time', sr=sr, ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Spectrogram')\n",
    "        fig.colorbar(img, ax=axes[0, 1], format='%+2.0f dB')\n",
    "        \n",
    "        # 3. MFCCs\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "        img = librosa.display.specshow(mfccs, x_axis='time', sr=sr, ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('MFCCs')\n",
    "        fig.colorbar(img, ax=axes[1, 0])\n",
    "        \n",
    "        # 4. Mel-spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        img = librosa.display.specshow(mel_spec_db, y_axis='mel', x_axis='time', \n",
    "                                      sr=sr, ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Mel-spectrogram')\n",
    "        fig.colorbar(img, ax=axes[1, 1], format='%+2.0f dB')\n",
    "        \n",
    "        # 5. Chromagram\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "        img = librosa.display.specshow(chroma, y_axis='chroma', x_axis='time', \n",
    "                                      sr=sr, ax=axes[2, 0])\n",
    "        axes[2, 0].set_title('Chromagram')\n",
    "        fig.colorbar(img, ax=axes[2, 0])\n",
    "        \n",
    "        # 6. Spectral contrast\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
    "        img = librosa.display.specshow(contrast, x_axis='time', sr=sr, ax=axes[2, 1])\n",
    "        axes[2, 1].set_title('Spectral Contrast')\n",
    "        fig.colorbar(img, ax=axes[2, 1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_feature_importance(model, feature_names, top_n=20):\n",
    "        \"\"\"Plot feature importance for tree-based models\"\"\"\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            indices = np.argsort(importances)[-top_n:]\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title(f'Top {top_n} Feature Importances')\n",
    "            plt.barh(range(top_n), importances[indices])\n",
    "            plt.yticks(range(top_n), [feature_names[i] for i in indices])\n",
    "            plt.xlabel('Relative Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Model doesn't have feature_importances_ attribute\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08014e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. MAIN EXECUTION AND EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function with example usage\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"ABNORMAL SOUND DETECTION SYSTEM\")\n",
    "    print(\"Detects: Gunshots, Car Crashes, Screams, Explosions, etc.\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize configuration\n",
    "    class SoundDetectionConfig:\n",
    "        def __init__(self):\n",
    "            # Audio parameters\n",
    "            self.sample_rate = 22050\n",
    "            self.n_mfcc = 40\n",
    "            self.n_fft = 2048\n",
    "            self.hop_length = 512\n",
    "\n",
    "            # Feature parameters\n",
    "            self.n_mels = 128\n",
    "            self.duration = 3.0\n",
    "\n",
    "            # Model parameters\n",
    "            self.random_state = 42\n",
    "\n",
    "    \n",
    "    # Initialize detector\n",
    "    detector = AbnormalSoundDetector(config)\n",
    "    \n",
    "    # Example 1: Prepare data from directory structure\n",
    "    # Directory should have subfolders named after CLASSES\n",
    "    data_dir = \"path/to/your/audio/dataset\"\n",
    "    \n",
    "    if os.path.exists(data_dir):\n",
    "        print(f\"\\nLoading data from {data_dir}...\")\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = detector.prepare_data(\n",
    "            data_directory=data_dir\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nUsing simulated data for demonstration...\")\n",
    "        # Create simulated data for demonstration\n",
    "        # In practice, replace this with your actual data loading\n",
    "        \n",
    "        # Simulate some audio file paths and labels\n",
    "        num_samples = 1000\n",
    "        audio_paths = [f\"simulated_audio_{i}.wav\" for i in range(num_samples)]\n",
    "        \n",
    "        # Simulate labels (adjust distribution as needed)\n",
    "        labels = np.random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8], \n",
    "                                size=num_samples,\n",
    "                                p=[0.3, 0.1, 0.2, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1])\n",
    "        \n",
    "        # For demonstration, we'll use simulated features\n",
    "        n_features = 500  # Approximate number of features\n",
    "        X_simulated = np.random.randn(num_samples, n_features)\n",
    "        y_simulated = labels\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_simulated, y_simulated, test_size=0.2, random_state=42\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.1, random_state=42\n",
    "        )\n",
    "    \n",
    "    # Train ensemble model\n",
    "    detector.train_ensemble_model(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    predictions, probabilities = detector.evaluate_ensemble(X_test, y_test)\n",
    "    \n",
    "    # Example 2: Predict single audio file\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SINGLE AUDIO PREDICTION EXAMPLE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_audio = \"path/to/test/audio.wav\"\n",
    "    if os.path.exists(test_audio):\n",
    "        result = detector.predict_single_audio(test_audio)\n",
    "        print(f\"\\nPrediction for {test_audio}:\")\n",
    "        print(f\"  Class: {result['predicted_class']}\")\n",
    "        print(f\"  Confidence: {result['confidence']:.2%}\")\n",
    "        print(f\"  Top 3 predictions:\")\n",
    "        for class_name, confidence in result['top_predictions']:\n",
    "            print(f\"    - {class_name}: {confidence:.2%}\")\n",
    "    else:\n",
    "        print(f\"\\nTest file {test_audio} not found.\")\n",
    "        print(\"Using simulated prediction for demonstration...\")\n",
    "        \n",
    "        # Simulate a prediction result\n",
    "        simulated_result = {\n",
    "            'predicted_class': 'gunshot',\n",
    "            'confidence': 0.87,\n",
    "            'top_predictions': [\n",
    "                ('gunshot', 0.87),\n",
    "                ('car_crash', 0.08),\n",
    "                ('explosion', 0.03)\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nSimulated prediction:\")\n",
    "        print(f\"  Class: {simulated_result['predicted_class']}\")\n",
    "        print(f\"  Confidence: {simulated_result['confidence']:.2%}\")\n",
    "        print(f\"  Top 3 predictions:\")\n",
    "        for class_name, confidence in simulated_result['top_predictions']:\n",
    "            print(f\"    - {class_name}: {confidence:.2%}\")\n",
    "    \n",
    "    # Example 3: Feature visualization\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE VISUALIZATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    visualizer = SoundVisualizer()\n",
    "    \n",
    "    # Plot feature importance for Random Forest\n",
    "    if 'random_forest' in detector.models:\n",
    "        # Create dummy feature names for demonstration\n",
    "        feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "\n",
    "        visualizer.plot_feature_importance(\n",
    "            detector.models['random_forest'],\n",
    "            feature_names,\n",
    "            top_n=15\n",
    "        )\n",
    "\n",
    "    \n",
    "    # Example 4: Real-time detection setup\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"REAL-TIME DETECTION SETUP\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nReal-time detection can be implemented using:\")\n",
    "    print(\"1. Microphone input with pyaudio\")\n",
    "    print(\"2. Network audio stream\")\n",
    "    print(\"3. Pre-recorded audio in chunks\")\n",
    "    print(\"\\nUse detector.real_time_detection() for implementation.\")\n",
    "    \n",
    "    # Save models for later use\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SAVING MODELS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    import joblib\n",
    "    import pickle\n",
    "    \n",
    "    # Save the detector object\n",
    "    with open('abnormal_sound_detector.pkl', 'wb') as f:\n",
    "        pickle.dump(detector, f)\n",
    "    \n",
    "    # Save individual models\n",
    "    for name, model in detector.models.items():\n",
    "        if name != 'deep_learning':  # Keras models need special handling\n",
    "            joblib.dump(model, f'{name}_model.joblib')\n",
    "    \n",
    "    print(\"Models saved successfully!\")\n",
    "    \n",
    "    return detector\n",
    "\n",
    "def load_and_use_saved_model():\n",
    "    \"\"\"Load a saved model and use it for prediction\"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    print(\"\\nLoading saved model...\")\n",
    "    \n",
    "    try:\n",
    "        with open('abnormal_sound_detector.pkl', 'rb') as f:\n",
    "            detector = pickle.load(f)\n",
    "        \n",
    "        print(\"Model loaded successfully!\")\n",
    "        \n",
    "        # Example usage\n",
    "        test_file = \"path/to/your/test_sound.wav\"\n",
    "        \n",
    "        if os.path.exists(test_file):\n",
    "            result = detector.predict_single_audio(test_file)\n",
    "            print(f\"\\nPrediction: {result['predicted_class']}\")\n",
    "            print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "            \n",
    "            if result['confidence'] > 0.7:\n",
    "                if result['predicted_class'] in ['gunshot', 'car_crash', 'explosion', 'scream']:\n",
    "                    print(f\"â ï¸  ALERT: Abnormal sound detected! ({result['predicted_class']})\")\n",
    "                    # You could trigger an alarm, send notification, etc.\n",
    "        else:\n",
    "            print(f\"Test file {test_file} not found.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"Saved model not found. Please train a model first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "321d10a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ABNORMAL SOUND DETECTION SYSTEM\n",
      "Detects: Gunshots, Car Crashes, Screams, Explosions, etc.\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SoundDetectionConfig' object has no attribute 'sample_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# EXECUTION\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Option 1: Train a new model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     detector = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Option 2: Load and use saved model\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# load_and_use_saved_model()\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Option 3: Adapt your existing code\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# X, y = prepare_dataset_from_existing_code()\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m         \u001b[38;5;28mself\u001b[39m.random_state = \u001b[32m42\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Initialize detector\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m detector = \u001b[43mAbnormalSoundDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Example 1: Prepare data from directory structure\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Directory should have subfolders named after CLASSES\u001b[39;00m\n\u001b[32m     36\u001b[39m data_dir = \u001b[33m\"\u001b[39m\u001b[33mpath/to/your/audio/dataset\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mAbnormalSoundDetector.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28mself\u001b[39m.feature_extractor = \u001b[43mAdvancedAudioFeatureExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mself\u001b[39m.preprocessor = AudioDataPreprocessor()\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mself\u001b[39m.models = {}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mAdvancedAudioFeatureExtractor.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28mself\u001b[39m.sample_rate = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_rate\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_mfcc = config.n_mfcc\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_fft = config.n_fft\n",
      "\u001b[31mAttributeError\u001b[39m: 'SoundDetectionConfig' object has no attribute 'sample_rate'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Option 1: Train a new model\n",
    "    detector = main()\n",
    "    \n",
    "    # Option 2: Load and use saved model\n",
    "    # load_and_use_saved_model()\n",
    "    \n",
    "    # Option 3: Adapt your existing code\n",
    "    # X, y = prepare_dataset_from_existing_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f498b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b6c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eae070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43f583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45f8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e30b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
