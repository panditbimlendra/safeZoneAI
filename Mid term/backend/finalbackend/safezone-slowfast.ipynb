{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:21:11.420859Z",
     "iopub.status.busy": "2025-08-10T07:21:11.420595Z",
     "iopub.status.idle": "2025-08-10T12:18:05.348425Z",
     "shell.execute_reply": "2025-08-10T12:18:05.345815Z",
     "shell.execute_reply.started": "2025-08-10T07:21:11.420834Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/kaggle/input/ucf-crime-full'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 324\u001b[39m\n\u001b[32m    321\u001b[39m parser.add_argument(\u001b[33m\"\u001b[39m\u001b[33m--experiment_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28mstr\u001b[39m, default=\u001b[33m\"\u001b[39m\u001b[33mslowfast_experiment\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m args = parser.parse_args()\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m CLASS_NAMES = \u001b[38;5;28msorted\u001b[39m([d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(os.path.join(args.data_dir, d))])\n\u001b[32m    325\u001b[39m NUM_CLASSES = \u001b[38;5;28mlen\u001b[39m(CLASS_NAMES)\n\u001b[32m    327\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDetected classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCLASS_NAMES\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: '/kaggle/input/ucf-crime-full'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import argparse\n",
    "from typing import List\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorchvideo.models.hub import slowfast_r50\n",
    "\n",
    "\n",
    "class UCFCrimeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir: str, class_names: List[str], transform=None, frames: int = 32):\n",
    "        self.root_dir = root_dir\n",
    "        self.class_names = class_names\n",
    "        self.transform = transform\n",
    "        self.frames = frames\n",
    "        self.samples = []\n",
    "\n",
    "        for label, cls in enumerate(class_names):\n",
    "            cls_dir = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(cls_dir):\n",
    "                continue\n",
    "            for vid in os.listdir(cls_dir):\n",
    "                if vid.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                    self.samples.append((os.path.join(cls_dir, vid), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        clip = self._load_video_cv2(path, self.frames)  # (C, T, H, W)\n",
    "\n",
    "        if self.transform:\n",
    "            clip = self.transform(clip)\n",
    "\n",
    "        return clip, int(label)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_video_cv2(path: str, frames: int):\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(f\"Failed to open video file {path}\")\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames <= 0:\n",
    "            raise RuntimeError(f\"Video {path} has zero frames\")\n",
    "\n",
    "        # Frame indices to sample evenly\n",
    "        if total_frames < frames:\n",
    "            indices = list(range(total_frames)) + [total_frames - 1] * (frames - total_frames)\n",
    "        else:\n",
    "            indices = np.linspace(0, total_frames - 1, frames).astype(int).tolist()\n",
    "\n",
    "        frames_list = []\n",
    "        frame_pos = 0\n",
    "        ret = True\n",
    "        sampled_idx = 0\n",
    "\n",
    "        for i in range(total_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if i == indices[sampled_idx]:\n",
    "                # Convert BGR to RGB, resize later in transform\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames_list.append(frame_rgb)\n",
    "                sampled_idx += 1\n",
    "                if sampled_idx >= len(indices):\n",
    "                    break\n",
    "        cap.release()\n",
    "\n",
    "        # If frames less than requested (rare), pad last frame\n",
    "        while len(frames_list) < frames:\n",
    "            frames_list.append(frames_list[-1])\n",
    "\n",
    "        # Convert to numpy array (T, H, W, C)\n",
    "        clip_np = np.stack(frames_list, axis=0)\n",
    "\n",
    "        # Convert to tensor (C, T, H, W)\n",
    "        clip_tensor = torch.from_numpy(clip_np).permute(3, 0, 1, 2).float() / 255.0\n",
    "\n",
    "        return clip_tensor\n",
    "\n",
    "\n",
    "class VideoTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, size=224):\n",
    "        super().__init__()\n",
    "        self.frame_transform = Compose([\n",
    "            Resize((256, 256)),\n",
    "            CenterCrop(size),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x shape: (C, T, H, W)\n",
    "        C, T, H, W = x.shape\n",
    "        x = x.permute(1, 0, 2, 3)  # (T, C, H, W)\n",
    "        frames = []\n",
    "        for i in range(T):\n",
    "            frame = x[i]\n",
    "            frame = self.frame_transform(frame)\n",
    "            frames.append(frame)\n",
    "        x = torch.stack(frames, dim=1)  # (C, T, H, W)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Lightning Module\n",
    "class SlowFastLitModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes: int, lr: float = 1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = slowfast_r50(pretrained=True)\n",
    "\n",
    "        in_features = self.model.blocks[-1].proj.in_features\n",
    "        self.model.blocks[-1].proj = nn.Linear(in_features, num_classes)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_precision = torchmetrics.Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "        self.val_recall = torchmetrics.Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "        self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _prepare_pathways(self, x: torch.Tensor):\n",
    "        slow = x[:, :, ::4, :, :]\n",
    "        fast = x\n",
    "        return [slow, fast]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x_in = self._prepare_pathways(x)\n",
    "        logits = self.forward(x_in)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.train_acc(preds, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x_in = self._prepare_pathways(x)\n",
    "        logits = self.forward(x_in)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.val_acc(preds, y)\n",
    "        precision = self.val_precision(preds, y)\n",
    "        recall = self.val_recall(preds, y)\n",
    "        f1 = self.val_f1(preds, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_precision\", precision, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_recall\", recall, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_f1\", f1, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}}\n",
    "\n",
    "\n",
    "# Data Module\n",
    "class UCFDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str, class_names: List[str], batch_size: int = 4, num_workers: int = 4, frames: int = 32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.class_names = class_names\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.frames = frames\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        transform = VideoTransform()\n",
    "        dataset = UCFCrimeDataset(self.data_dir, self.class_names, transform=transform, frames=self.frames)\n",
    "        n = len(dataset)\n",
    "        train_n = int(0.8 * n)\n",
    "        val_n = n - train_n\n",
    "        self.train_ds, self.val_ds = random_split(\n",
    "            dataset, [train_n, val_n], generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=self.num_workers, pin_memory=torch.cuda.is_available(), prefetch_factor=2)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, pin_memory=torch.cuda.is_available(), prefetch_factor=2)\n",
    "\n",
    "\n",
    "# Callback for metrics plotting\n",
    "class MetricsPlotter(pl.Callback):\n",
    "    def __init__(self, out_dir=\"logs/plots\"):\n",
    "        super().__init__()\n",
    "        self.out_dir = out_dir\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_acc\": [],\n",
    "            \"val_precision\": [],\n",
    "            \"val_recall\": [],\n",
    "            \"val_f1\": [],\n",
    "        }\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "\n",
    "        def _get(k):\n",
    "            return float(metrics[k]) if k in metrics else None\n",
    "\n",
    "        self.history[\"val_loss\"].append(_get(\"val_loss\"))\n",
    "        self.history[\"val_acc\"].append(_get(\"val_acc\"))\n",
    "        self.history[\"val_precision\"].append(_get(\"val_precision\"))\n",
    "        self.history[\"val_recall\"].append(_get(\"val_recall\"))\n",
    "        self.history[\"val_f1\"].append(_get(\"val_f1\"))\n",
    "        self.history[\"train_loss\"].append(_get(\"train_loss\"))\n",
    "        self.history[\"train_acc\"].append(_get(\"train_acc\"))\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        epochs = range(1, len(self.history[\"val_loss\"]) + 1)\n",
    "\n",
    "        plt.figure()\n",
    "        if any(x is not None for x in self.history[\"train_loss\"]):\n",
    "            plt.plot(epochs, self.history[\"train_loss\"], label=\"train_loss\")\n",
    "        if any(x is not None for x in self.history[\"val_loss\"]):\n",
    "            plt.plot(epochs, self.history[\"val_loss\"], label=\"val_loss\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Loss per epoch\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(self.out_dir, \"loss.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        if any(x is not None for x in self.history[\"train_acc\"]):\n",
    "            plt.plot(epochs, self.history[\"train_acc\"], label=\"train_acc\")\n",
    "        if any(x is not None for x in self.history[\"val_acc\"]):\n",
    "            plt.plot(epochs, self.history[\"val_acc\"], label=\"val_acc\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Accuracy per epoch\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(self.out_dir, \"accuracy.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        if any(x is not None for x in self.history[\"val_precision\"]):\n",
    "            plt.plot(epochs, self.history[\"val_precision\"], label=\"precision\")\n",
    "        if any(x is not None for x in self.history[\"val_recall\"]):\n",
    "            plt.plot(epochs, self.history[\"val_recall\"], label=\"recall\")\n",
    "        if any(x is not None for x in self.history[\"val_f1\"]):\n",
    "            plt.plot(epochs, self.history[\"val_f1\"], label=\"f1\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"score\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Precision / Recall / F1\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(self.out_dir, \"prf1.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Saved plots to {self.out_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.argv = [\n",
    "        \"notebook_script\",\n",
    "        \"--data_dir\", \"/kaggle/input/ucf-crime-full\",\n",
    "        \"--batch_size\", \"2\",\n",
    "        \"--epochs\", \"20\",\n",
    "        \"--num_workers\", \"4\",\n",
    "        \"--lr\", \"0.0001\",\n",
    "        \"--frames\", \"32\",\n",
    "        \"--log_dir\", \"logs\",\n",
    "        \"--experiment_name\", \"slowfast_experiment\"\n",
    "    ]\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", type=str, required=True)\n",
    "    # ... rest of the code\n",
    "\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", type=str, required=True)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--frames\", type=int, default=32)\n",
    "    parser.add_argument(\"--log_dir\", type=str, default=\"logs\")\n",
    "    parser.add_argument(\"--experiment_name\", type=str, default=\"slowfast_experiment\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    CLASS_NAMES = sorted([d for d in os.listdir(args.data_dir) if os.path.isdir(os.path.join(args.data_dir, d))])\n",
    "    NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "    print(f\"Detected classes: {CLASS_NAMES}\")\n",
    "    print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "\n",
    "    dm = UCFDataModule(args.data_dir, CLASS_NAMES, batch_size=args.batch_size, num_workers=args.num_workers, frames=args.frames)\n",
    "    model = SlowFastLitModel(num_classes=NUM_CLASSES, lr=args.lr)\n",
    "\n",
    "    logger = CSVLogger(save_dir=args.log_dir, name=args.experiment_name)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_acc\", mode=\"max\", save_top_k=1,\n",
    "                                          filename=\"best-checkpoint-{epoch:02d}-{val_acc:.4f}\")\n",
    "\n",
    "    metrics_plotter = MetricsPlotter(out_dir=os.path.join(args.log_dir, args.experiment_name, \"plots\"))\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=args.epochs,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=torch.cuda.device_count() if torch.cuda.is_available() else 1,\n",
    "        precision=\"16-mixed\" if torch.cuda.is_available() else 32,\n",
    "        log_every_n_steps=10,\n",
    "        logger=logger,\n",
    "        callbacks=[checkpoint_callback, metrics_plotter],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T09:41:55.848168Z",
     "iopub.status.busy": "2025-08-11T09:41:55.847849Z",
     "iopub.status.idle": "2025-08-11T09:41:59.147878Z",
     "shell.execute_reply": "2025-08-11T09:41:59.146870Z",
     "shell.execute_reply.started": "2025-08-11T09:41:55.848145Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:119: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:119: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\Bimlendra\\AppData\\Local\\Temp\\ipykernel_24284\\688474330.py:119: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  results = inference.predict_batch([\"backend\\finalbackend\\carcrash1.mp4\"], batch_size=2)\n",
      "C:\\Users\\Bimlendra\\AppData\\Local\\Temp\\ipykernel_24284\\688474330.py:119: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  results = inference.predict_batch([\"backend\\finalbackend\\carcrash1.mp4\"], batch_size=2)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'c:/Users/Bimlendra/OneDrive/Desktop/safeZoneAI/Mid term/backend/finalbackend/backend\\x0cinalbackend\\x08est-checkpoint-epoch=06-val_acc=0.4500.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m    109\u001b[39m checkpoint_path = \u001b[33m\"\u001b[39m\u001b[33mbackend\u001b[39m\u001b[38;5;130;01m\\f\u001b[39;00m\u001b[33minalbackend\u001b[39m\u001b[38;5;130;01m\\b\u001b[39;00m\u001b[33mest-checkpoint-epoch=06-val_acc=0.4500.ckpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m class_names = [\n\u001b[32m    111\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mAbuse\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mArrest\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mArson\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAssault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBurglary\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    112\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mExplosion\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFighting\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNormal_Videos_for_Event_Recognition\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    113\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mRoadAccidents\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mRobbery\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mShooting\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mShoplifting\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    114\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mStealing\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mVandalism\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    115\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m inference = \u001b[43mSlowFastVideoInference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m results = inference.predict_batch([\u001b[33m\"\u001b[39m\u001b[33mbackend\u001b[39m\u001b[38;5;130;01m\\f\u001b[39;00m\u001b[33minalbackend\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mcarcrash1.mp4\u001b[39m\u001b[33m\"\u001b[39m], batch_size=\u001b[32m2\u001b[39m)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mSlowFastVideoInference.__init__\u001b[39m\u001b[34m(self, checkpoint_path, class_names, frames, device)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mself\u001b[39m.frames = frames\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m.class_names = class_names\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mSlowFastLitModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n\u001b[32m     69\u001b[39m \u001b[38;5;28mself\u001b[39m.model.to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:130\u001b[39m, in \u001b[36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    127\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_type.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.method.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` cannot be called on an instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1781\u001b[39m, in \u001b[36mLightningModule.load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, weights_only, **kwargs)\u001b[39m\n\u001b[32m   1686\u001b[39m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_checkpoint\u001b[39m(\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1694\u001b[39m     **kwargs: Any,\n\u001b[32m   1695\u001b[39m ) -> Self:\n\u001b[32m   1696\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[32m   1697\u001b[39m \u001b[33;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[32m   1698\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1779\u001b[39m \n\u001b[32m   1780\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1781\u001b[39m     loaded = \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1790\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:65\u001b[39m, in \u001b[36m_load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, weights_only, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m map_location = map_location \u001b[38;5;129;01mor\u001b[39;00m _default_map_location\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     checkpoint = \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[32m     68\u001b[39m checkpoint = _pl_migrate_checkpoint(\n\u001b[32m     69\u001b[39m     checkpoint, checkpoint_path=(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     70\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:72\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(path_or_url, map_location, weights_only)\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.hub.load_state_dict_from_url(\n\u001b[32m     67\u001b[39m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[32m     68\u001b[39m         map_location=map_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     69\u001b[39m         weights_only=weights_only,\n\u001b[32m     70\u001b[39m     )\n\u001b[32m     71\u001b[39m fs = get_filesystem(path_or_url)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.load(\n\u001b[32m     74\u001b[39m         f,\n\u001b[32m     75\u001b[39m         map_location=map_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     76\u001b[39m         weights_only=weights_only,\n\u001b[32m     77\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fsspec\\spec.py:1349\u001b[39m, in \u001b[36mAbstractFileSystem.open\u001b[39m\u001b[34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1348\u001b[39m     ac = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mautocommit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._intrans)\n\u001b[32m-> \u001b[39m\u001b[32m1349\u001b[39m     f = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1358\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfsspec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fsspec\\implementations\\local.py:214\u001b[39m, in \u001b[36mLocalFileSystem._open\u001b[39m\u001b[34m(self, path, mode, block_size, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28mself\u001b[39m.makedirs(\u001b[38;5;28mself\u001b[39m._parent(path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fsspec\\implementations\\local.py:391\u001b[39m, in \u001b[36mLocalFileOpener.__init__\u001b[39m\u001b[34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28mself\u001b[39m.compression = get_compression(path, compression)\n\u001b[32m    390\u001b[39m \u001b[38;5;28mself\u001b[39m.blocksize = io.DEFAULT_BUFFER_SIZE\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fsspec\\implementations\\local.py:396\u001b[39m, in \u001b[36mLocalFileOpener._open\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f.closed:\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.autocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode:\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m         \u001b[38;5;28mself\u001b[39m.f = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compression:\n\u001b[32m    398\u001b[39m             compress = compr[\u001b[38;5;28mself\u001b[39m.compression]\n",
      "\u001b[31mOSError\u001b[39m: [Errno 22] Invalid argument: 'c:/Users/Bimlendra/OneDrive/Desktop/safeZoneAI/Mid term/backend/finalbackend/backend\\x0cinalbackend\\x08est-checkpoint-epoch=06-val_acc=0.4500.ckpt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, video_paths, transform, frames=32):\n",
    "        self.video_paths = video_paths\n",
    "        self.transform = transform\n",
    "        self.frames = frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        clip = self._load_video_cv2(video_path, self.frames)\n",
    "        if self.transform:\n",
    "            clip = self.transform(clip)\n",
    "        return clip, video_path\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_video_cv2(path: str, frames: int):\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(f\"Cannot open video {path}\")\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames <= 0:\n",
    "            raise RuntimeError(f\"Video {path} has no frames\")\n",
    "\n",
    "        if total_frames < frames:\n",
    "            indices = list(range(total_frames)) + [total_frames - 1] * (frames - total_frames)\n",
    "        else:\n",
    "            indices = np.linspace(0, total_frames - 1, frames).astype(int).tolist()\n",
    "\n",
    "        frames_list = []\n",
    "        sampled_idx = 0\n",
    "\n",
    "        for i in range(total_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if i == indices[sampled_idx]:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames_list.append(frame_rgb)\n",
    "                sampled_idx += 1\n",
    "                if sampled_idx >= len(indices):\n",
    "                    break\n",
    "        cap.release()\n",
    "\n",
    "        while len(frames_list) < frames:\n",
    "            frames_list.append(frames_list[-1])\n",
    "\n",
    "        clip_np = np.stack(frames_list, axis=0)  # (T, H, W, C)\n",
    "        clip_tensor = torch.from_numpy(clip_np).permute(3, 0, 1, 2).float() / 255.0  # (C, T, H, W)\n",
    "        return clip_tensor\n",
    "\n",
    "\n",
    "class SlowFastVideoInference:\n",
    "    def __init__(self, checkpoint_path: str, class_names: list, frames: int = 32, device: str = None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.frames = frames\n",
    "        self.class_names = class_names\n",
    "\n",
    "        self.model = SlowFastLitModel.load_from_checkpoint(checkpoint_path, num_classes=len(class_names))\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.transform = VideoTransform()\n",
    "\n",
    "    def _prepare_pathways(self, x: torch.Tensor):\n",
    "        # x shape: (B, C, T, H, W)\n",
    "        slow = x[:, :, ::4, :, :]  # slow pathway: sample every 4th frame\n",
    "        fast = x                   # fast pathway: all frames\n",
    "        return [slow, fast]\n",
    "\n",
    "    def predict_batch(self, video_paths: list, batch_size=4, num_workers=4):\n",
    "        dataset = InferenceDataset(video_paths, transform=self.transform, frames=self.frames)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for clips, paths in loader:\n",
    "                # clips shape: (B, C, T, H, W)\n",
    "                clips = clips.to(self.device)\n",
    "                inputs = self._prepare_pathways(clips)  # list of two tensors [slow, fast]\n",
    "                logits = self.model(inputs)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                pred_idxs = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "                probs_np = probs.cpu().numpy()\n",
    "\n",
    "                for path, pred_idx, prob in zip(paths, pred_idxs, probs_np):\n",
    "                    results.append({\n",
    "                        \"video_path\": path,\n",
    "                        \"predicted_class\": self.class_names[pred_idx],\n",
    "                        \"predicted_prob\": prob[pred_idx]\n",
    "                    })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def predict(self, video_path: str):\n",
    "        return self.predict_batch([video_path], batch_size=1)[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "checkpoint_path = \"backend\\finalbackend\\best-checkpoint-epoch=06-val_acc=0.4500.ckpt\"\n",
    "class_names = [\n",
    "    'Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary',\n",
    "    'Explosion', 'Fighting', 'Normal_Videos_for_Event_Recognition',\n",
    "    'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting',\n",
    "    'Stealing', 'Vandalism'\n",
    "]\n",
    "\n",
    "inference = SlowFastVideoInference(checkpoint_path, class_names)\n",
    "\n",
    "results = inference.predict_batch([\"backend\\finalbackend\\carcrash1.mp4\"], batch_size=2)\n",
    "\n",
    "for res in results:\n",
    "    print(f\"Video: {res['video_path']}\")\n",
    "    print(f\"Predicted class: {res['predicted_class']}\")\n",
    "    print(f\"Probability: {res['predicted_prob']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T08:19:52.894573Z",
     "iopub.status.busy": "2025-08-11T08:19:52.893509Z",
     "iopub.status.idle": "2025-08-11T08:21:53.318807Z",
     "shell.execute_reply": "2025-08-11T08:21:53.318036Z",
     "shell.execute_reply.started": "2025-08-11T08:19:52.894544Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:/kaggle/input/slowfast/pytorch/default/1/best-checkpoint-epoch06-val_acc0.4500.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m checkpoint_path = \u001b[33m\"\u001b[39m\u001b[33m/kaggle/input/slowfast/pytorch/default/1/best-checkpoint-epoch06-val_acc0.4500.ckpt\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# <-- change\u001b[39;00m\n\u001b[32m     22\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m model = \u001b[43mSlowFastLitModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m model.eval()\n\u001b[32m     26\u001b[39m model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:130\u001b[39m, in \u001b[36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    127\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_type.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.method.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` cannot be called on an instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1781\u001b[39m, in \u001b[36mLightningModule.load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, weights_only, **kwargs)\u001b[39m\n\u001b[32m   1686\u001b[39m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_checkpoint\u001b[39m(\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1694\u001b[39m     **kwargs: Any,\n\u001b[32m   1695\u001b[39m ) -> Self:\n\u001b[32m   1696\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[32m   1697\u001b[39m \u001b[33;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[32m   1698\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1779\u001b[39m \n\u001b[32m   1780\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1781\u001b[39m     loaded = \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1790\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:65\u001b[39m, in \u001b[36m_load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, weights_only, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m map_location = map_location \u001b[38;5;129;01mor\u001b[39;00m _default_map_location\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     checkpoint = \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[32m     68\u001b[39m checkpoint = _pl_migrate_checkpoint(\n\u001b[32m     69\u001b[39m     checkpoint, checkpoint_path=(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     70\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:72\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(path_or_url, map_location, weights_only)\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.hub.load_state_dict_from_url(\n\u001b[32m     67\u001b[39m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[32m     68\u001b[39m         map_location=map_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     69\u001b[39m         weights_only=weights_only,\n\u001b[32m     70\u001b[39m     )\n\u001b[32m     71\u001b[39m fs = get_filesystem(path_or_url)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.load(\n\u001b[32m     74\u001b[39m         f,\n\u001b[32m     75\u001b[39m         map_location=map_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     76\u001b[39m         weights_only=weights_only,\n\u001b[32m     77\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fsspec\\spec.py:1349\u001b[39m, in \u001b[36mAbstractFileSystem.open\u001b[39m\u001b[34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1348\u001b[39m     ac = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mautocommit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._intrans)\n\u001b[32m-> \u001b[39m\u001b[32m1349\u001b[39m     f = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1358\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfsspec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fsspec\\implementations\\local.py:214\u001b[39m, in \u001b[36mLocalFileSystem._open\u001b[39m\u001b[34m(self, path, mode, block_size, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28mself\u001b[39m.makedirs(\u001b[38;5;28mself\u001b[39m._parent(path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fsspec\\implementations\\local.py:391\u001b[39m, in \u001b[36mLocalFileOpener.__init__\u001b[39m\u001b[34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28mself\u001b[39m.compression = get_compression(path, compression)\n\u001b[32m    390\u001b[39m \u001b[38;5;28mself\u001b[39m.blocksize = io.DEFAULT_BUFFER_SIZE\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bimlendra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fsspec\\implementations\\local.py:396\u001b[39m, in \u001b[36mLocalFileOpener._open\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f.closed:\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.autocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode:\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m         \u001b[38;5;28mself\u001b[39m.f = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compression:\n\u001b[32m    398\u001b[39m             compress = compr[\u001b[38;5;28mself\u001b[39m.compression]\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:/kaggle/input/slowfast/pytorch/default/1/best-checkpoint-epoch06-val_acc0.4500.ckpt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, top_k_accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \"/kaggle/input/ucf-crime-full\"  \n",
    "class_names = ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'Normal_Videos_for_Event_Recognition', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']  # same list you used during training\n",
    "\n",
    "transform = VideoTransform()\n",
    "dm = UCFDataModule(data_dir=data_dir, class_names=class_names, batch_size=4, num_workers=4, frames=32)\n",
    "dm.setup(stage=\"validate\")\n",
    "val_loader = dm.val_dataloader()\n",
    "\n",
    "\n",
    "checkpoint_path = \"/kaggle/input/slowfast/pytorch/default/1/best-checkpoint-epoch06-val_acc0.4500.ckpt\"  # <-- change\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SlowFastLitModel.load_from_checkpoint(checkpoint_path, num_classes=len(class_names))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Inference loop on validation split\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for clips, labels in val_loader:\n",
    "        slow = clips[:, :, ::4, :, :].to(device)\n",
    "        fast = clips.to(device)\n",
    "        outputs = model([slow, fast])\n",
    "\n",
    "\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "#  Metrics on validation split\n",
    "top1 = accuracy_score(all_labels, all_preds)\n",
    "top5 = top_k_accuracy_score(all_labels, all_probs, k=5)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "weighted_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f\"Top-1 Accuracy: {top1*100:.2f}%\")\n",
    "print(f\"Top-5 Accuracy: {top5*100:.2f}%\")\n",
    "print(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1-score: {weighted_f1:.4f}\")\n",
    "\n",
    "# Per-class precision/recall/F1\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report.to_csv(\"slowfast_classification_report.csv\")\n",
    "print(\"Per-class metrics saved to slowfast_classification_report.csv\")\n",
    "\n",
    "#  Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - SlowFast UCF-Crime\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"slowfast_confusion_matrix.png\", dpi=300)\n",
    "plt.close()\n",
    "print(\"Confusion matrix saved to slowfast_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn   \n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "from pytorchvideo.models.hub import slowfast_r50\n",
    "\n",
    "\n",
    "class VideoTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, size=224):\n",
    "        super().__init__()\n",
    "        self.frame_transform = Compose([\n",
    "            Resize((256, 256)),\n",
    "            CenterCrop(size),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x shape: (C, T, H, W)\n",
    "        C, T, H, W = x.shape\n",
    "        x = x.permute(1, 0, 2, 3)  # (T, C, H, W)\n",
    "        frames = []\n",
    "        for i in range(T):\n",
    "            frame = x[i]\n",
    "            frame = self.frame_transform(frame)\n",
    "            frames.append(frame)\n",
    "        x = torch.stack(frames, dim=1)  # (C, T, H, W)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Lightning Module\n",
    "class SlowFastLitModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes: int, lr: float = 1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = slowfast_r50(pretrained=True)\n",
    "\n",
    "        in_features = self.model.blocks[-1].proj.in_features\n",
    "        self.model.blocks[-1].proj = nn.Linear(in_features, num_classes)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_precision = torchmetrics.Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "        self.val_recall = torchmetrics.Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "        self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _prepare_pathways(self, x: torch.Tensor):\n",
    "        slow = x[:, :, ::4, :, :]\n",
    "        fast = x\n",
    "        return [slow, fast]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x_in = self._prepare_pathways(x)\n",
    "        logits = self.forward(x_in)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.train_acc(preds, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x_in = self._prepare_pathways(x)\n",
    "        logits = self.forward(x_in)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.val_acc(preds, y)\n",
    "        precision = self.val_precision(preds, y)\n",
    "        recall = self.val_recall(preds, y)\n",
    "        f1 = self.val_f1(preds, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_precision\", precision, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_recall\", recall, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_f1\", f1, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1242192,
     "sourceId": 2072349,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8030996,
     "sourceId": 12707116,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 424965,
     "modelInstanceId": 407074,
     "sourceId": 515405,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 425391,
     "modelInstanceId": 407521,
     "sourceId": 516578,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
